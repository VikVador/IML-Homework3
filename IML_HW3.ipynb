{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e8531373-cd83-40d0-bbc2-28b641c208e6",
   "metadata": {},
   "source": [
    "<hr style=\"color:#a4342d;\">\n",
    "<p align=\"center\">\n",
    "    <b style=\"font-size:2.5vw; color:#a4342d; font-weight:bold;\">\n",
    "    Introduction to machine learning - Homework 3\n",
    "    </b>\n",
    "</p>\n",
    "<hr style=\"color:#a4342d;\">\n",
    "\n",
    "<b>Authors</b>: <i>C. Bosch, M. Cornet & V. Mangeleer</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae3b7eac",
   "metadata": {},
   "source": [
    "[comment]: <> (Section)\n",
    "<hr style=\"color:#a4342d;\">\n",
    "<p align=\"center\">\n",
    "    <b style=\"font-size:1.5vw; color:#a4342d;\">\n",
    "    Initialization\n",
    "    </b>\n",
    "</p>\n",
    "<hr style=\"color:#a4342d;\">\n",
    "\n",
    "[comment]: <> (Description)\n",
    "<p align=\"justify\">\n",
    "    In this section, one will be able to initialize all the librairies needed. In addition to that, the untouched dataset will be loaded and inspected with an histogram made available by the incredbile Pandas librairy !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae5ae0ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- LIBRAIRIES --\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sn\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Allow notebook to plot in terminal\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b90a09b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- FUNCTION --\n",
    "\n",
    "# Used to print a basic section title in terminal\n",
    "def section(title = \"UNKNOWN\"):\n",
    "\n",
    "    # Number of letters to determine section size\n",
    "    title_size = len(title)\n",
    "\n",
    "    # Section title boundaries\n",
    "    boundary  = \"-\"\n",
    "    for i in range(title_size + 1):\n",
    "        boundary += \"-\"\n",
    "    \n",
    "    # Printing section\n",
    "    print(boundary)\n",
    "    print(f\" {title} \")\n",
    "    print(boundary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5da1c00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- ORIGINAL DATASET --\n",
    "# The original dataset is contained in the \"data/original\" folder\n",
    "\n",
    "# Stores the original dataset\n",
    "dataset_original_X = []\n",
    "dataset_original_Y = []\n",
    "\n",
    "# Load the original dataset\n",
    "for i in range(1, 11):\n",
    "    dataset_original_X.append(pd.read_csv(f\"data/original/X_Zone_{i}.csv\"))\n",
    "    dataset_original_Y.append(pd.read_csv(f\"data/original/Y_Zone_{i}.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3e0c488",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- BASIC INFORMATION DATASET --\n",
    "\n",
    "# Loading X and Y dataset for the first wind turbine\n",
    "dataset_X1 = dataset_original_X[0]\n",
    "dataset_Y1 = dataset_original_Y[0]\n",
    "\n",
    "# Displaying their relative information\n",
    "section(\"WIND TURBINE 1 - X Dataset\")\n",
    "section(\"HEAD\")\n",
    "print(dataset_X1.head())\n",
    "section(\"INFO\")\n",
    "dataset_X1.info()\n",
    "\n",
    "section(\"WIND TURBINE 1 - Y Dataset\")\n",
    "section(\"HEAD\")\n",
    "print(dataset_Y1.head())\n",
    "section(\"INFO\")\n",
    "dataset_Y1.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b8fbd5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- HISTOGRAM --\n",
    "\n",
    "# Extracting only relevant variables\n",
    "dataset_X1_relevant = dataset_X1[[\"U10\", \"U100\", \"V10\", \"V100\"]]\n",
    "dataset_X1_relevant.hist(bins = 30, sharex = True, figsize = (20, 15))\n",
    "\n",
    "dataset_Y1_relevant = dataset_Y1[[\"TARGETVAR\"]]\n",
    "dataset_Y1_relevant.hist(bins = 20)\n",
    "\n",
    "# Note (I forgot):\n",
    "# x-axis = speed interval in [m/s]\n",
    "# y_axis = number of instances in that interval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "708c306f",
   "metadata": {},
   "source": [
    "[comment]: <> (Section)\n",
    "<hr style=\"color:#a4342d;\"></hr>\n",
    "<p align=\"center\">\n",
    "    <b style=\"font-size:1.5vw; color:#a4342d;\">\n",
    "    Dataset - Train & Test | Pipeline\n",
    "    </b>\n",
    "</p>\n",
    "<hr style=\"color:#a4342d;\"></hr>\n",
    "\n",
    "[comment]: <> (Description)\n",
    "<p align=\"justify\">\n",
    "    In this section, one will be able to explore further the dataset ! First, one needs to create a train and test set. Then, it is interesitng to look for correlations, new variables and possible improvements to the current dataset. All the new datasets will be save in the datafolder and ready to use by our different models ! As a first intuition, I think it might be interesting to add more data in each dataset. Indeed, it could be interesting to add to each sample of the X set (one of these attributes or all of them, we must make some tests :D):\n",
    "    <ul>\n",
    "    <li>Libraries;</li>\n",
    "    <li>Functions.</li>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ae5ae0ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- FUNCTIONS : MEAN, VARIANCE, ZONAL AVERAGE SPEED AND TIME STEPS --\n",
    "#\n",
    "# Used to compute the mean and variance of a variable over some timeslices in the dataset\n",
    "def computeMeanVariance(datasets, \n",
    "                        variables = [\"U100\"     , \"V100\"],\n",
    "                        names_m   = [\"U100_mean\", \"V100_mean\"],\n",
    "                        names_v   = [\"U100_var\" , \"V100_var\"],\n",
    "                        window    = 100):\n",
    "\n",
    "    # Security\n",
    "    assert len(variables) == len(names_m), \"Each variable should have a name for its mean\"\n",
    "    assert len(variables) == len(names_v), \"Each variable should have a name for its var\"\n",
    "    assert window > 1, \"Window size must be greater than 1 to compute mean and var\"\n",
    "\n",
    "    # Looping over all the datasets\n",
    "    for d in datasets:\n",
    "\n",
    "        # Looping over the variables whose mean and var must be computed\n",
    "        for v, nm, nv in zip(variables, names_m, names_v):\n",
    "\n",
    "            # Retreiving data \n",
    "            data = d.loc[: , [v]].to_numpy()\n",
    "\n",
    "            # Stores mean and variance (1st and 2nd : mean = their value, var = 0 otherwise NAN problem while computation)\n",
    "            mean = [data[0][0], data[1][0]]\n",
    "            var  = [0, 0]\n",
    "\n",
    "            for i in range(2, len(data)):\n",
    "\n",
    "                # Start and end index for computation\n",
    "                index_start = i - window if i - window >= 0 else 0\n",
    "                index_end   = i - 1 if i - 1 >= 0 else 0\n",
    "\n",
    "                # Computing mean and variance (much faster using numpy variables)\n",
    "                mean.append(np.mean(data[index_start:index_end]))\n",
    "                var.append(np.var(data[index_start:index_end]))\n",
    "            \n",
    "            # Adding new data to dataset\n",
    "            d[nm] = mean\n",
    "            d[nv] = var\n",
    "\n",
    "# Used to compute the instantenous mean and variance of a variable accross multiple datasets\n",
    "def computeZonalValue(datasets, \n",
    "                      variables = [\"U100\"           , \"V100\"],\n",
    "                      names_m   = [\"U100_zonal_mean\", \"V100_zonal_mean\"],\n",
    "                      names_v   = [\"U100_zonal_var\" , \"V100_zonal_var\"]):\n",
    "\n",
    "    # Security\n",
    "    assert len(variables) == len(names_m), \"Each variable should have a name for its zonal mean\"\n",
    "    assert len(variables) == len(names_v), \"Each variable should have a name for its zonal var\"\n",
    "    assert len(datasets) > 1, \"To compute mean and var, at least 2 datasets are needed\"\n",
    "\n",
    "    # Looping over the variables whose mean and var must be computed\n",
    "    for v, nm, nv in zip(variables, names_m, names_v):\n",
    "\n",
    "        # Number of samples\n",
    "        nb_samples = len(datasets[0])\n",
    "\n",
    "        # Stores all the different values in numpy matrix for efficient computation\n",
    "        data = np.zeros((nb_samples, len(datasets)))\n",
    "\n",
    "        # Retreiving all the corresponding data\n",
    "        for i, d in enumerate(datasets):\n",
    "            \n",
    "            # Squeeze is there to remove useless dimension (Ask Victor)\n",
    "            data[:, i] = np.squeeze(d.loc[: , [v]].to_numpy())\n",
    "\n",
    "        # Computing mean and variance (much faster using numpy variables)\n",
    "        mean = np.mean(data, axis = 1) # Axis = 1 to make mean over each row\n",
    "        var  = np.var(data, axis = 1)\n",
    "\n",
    "        # Adding new data to all the datasets\n",
    "        for d in datasets:\n",
    "            d[nm] = mean\n",
    "            d[nv] = var\n",
    "\n",
    "# Used to normalize the data of different variables\n",
    "def normalize(datasets,\n",
    "              variables = [\"U100\", \"V100\"]):\n",
    "\n",
    "    # Looping over all the datasets\n",
    "    for d in datasets:\n",
    "\n",
    "        # Looping over the different columns\n",
    "        for i, c in enumerate(d.columns):\n",
    "\n",
    "            # Check if the column must be normalized\n",
    "            if c in variables:\n",
    "                \n",
    "                # Retreiving data\n",
    "                data = d.iloc[:, i].to_numpy()\n",
    "\n",
    "                # Normalizing the column\n",
    "                d.iloc[:, i] = (data - np.mean(data))/np.std(data)\n",
    "              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a3e2ef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- DATA LOADER -- \n",
    "# This class has for purpose to handle the data and make our life easier ! You will be able to:\n",
    "# - Store the original dataset (a list of .csv for X and another for Y)\n",
    "# - Define and apply a pipeline on the data and store a copy of it (leaving original data untouched)\n",
    "# - Combine all the small datasets into a big one\n",
    "# - Split the data into a train and test set\n",
    "# - Save your new dataset\n",
    "# - Load a dataset\n",
    "\n",
    "import copy\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "class dataLoader():\n",
    "    \n",
    "    # Initialization of the loader\n",
    "    def __init__(self, datasets_X, datasets_Y):\n",
    "\n",
    "        # Stores the original, transformed and final datasets\n",
    "        self.original_datasets_X    = datasets_X\n",
    "        self.original_datasets_Y    = datasets_Y\n",
    "        self.transformed_datasets_X = datasets_X\n",
    "        self.transformed_datasets_Y = datasets_Y\n",
    "        self.final_dataset_X        = None\n",
    "        self.final_dataset_Y        = None\n",
    "\n",
    "    # Used to display the head of the transformed dataset (first set)\n",
    "    def showHeadTransformed(self):\n",
    "        section(\"Dataset - X - Transformed\")\n",
    "        print(self.transformed_datasets_X[0].head())\n",
    "        section(\"Dataset - Y - Transformed\")\n",
    "        print(self.transformed_datasets_Y[0].head())\n",
    "\n",
    "    # Used to display the head of the final dataset (first set)\n",
    "    def showHeadFinal(self):\n",
    "        section(\"Dataset - X - Final\")\n",
    "        print(self.final_dataset_X.head())\n",
    "        section(\"Dataset - Y - Final\")\n",
    "        print(self.final_dataset_Y.head())\n",
    "\n",
    "    # Used to split the final dataset into a trial and test set\n",
    "    def splitTrainTest(self, numpy = False, test_size = 0.3):\n",
    "\n",
    "        # Computing train and test sets\n",
    "        train_X, test_X, train_Y, test_Y = train_test_split(self.final_dataset_X, self.final_dataset_Y, test_size = test_size)\n",
    "\n",
    "        # Numpy conversion\n",
    "        if numpy:\n",
    "            train_X = train_X.to_numpy()\n",
    "            train_Y = train_Y.to_numpy()\n",
    "            test_X  = test_X.to_numpy()\n",
    "            test_Y  = test_Y.to_numpy()\n",
    "\n",
    "        return train_X, test_X, train_Y, test_Y\n",
    "        \n",
    "    # Used to combine all the transformed datasets into a big one\n",
    "    def combine(self):\n",
    "        self.final_dataset_X = pd.concat(self.transformed_datasets_X)\n",
    "        self.final_dataset_Y = pd.concat(self.transformed_datasets_Y)\n",
    "\n",
    "    # Used to save the final dataset in .csv format for later\n",
    "    def save(self, name = \"dataset_1\"):\n",
    "        self.final_dataset_X.to_csv(f\"data/{name}_X.csv\")\n",
    "        self.final_dataset_X.to_csv(f\"data/{name}_Y.csv\")\n",
    "\n",
    "    #--------------------------------------------------------------------------------\n",
    "    #                                    PIPELINES\n",
    "    #--------------------------------------------------------------------------------\n",
    "    #\n",
    "    # TYPE 1 - Normalization, Average and Var, Zonal of U100 and V100\n",
    "    def pipeline_1(self, window   = 10,\n",
    "                         var      = [\"U100\", \"V100\"],\n",
    "                         n_aver   = [\"U100_mean\", \"V100_mean\"],\n",
    "                         n_var    = [\"U100_var\" , \"V100_var\"],\n",
    "                         n_z_aver = [\"U100_zonal_mean\", \"V100_zonal_mean\"],\n",
    "                         n_z_var  = [\"U100_zonal_var\" , \"V100_zonal_var\"]):\n",
    "\n",
    "        # Copying original dataset\n",
    "        dX = copy.deepcopy(self.original_datasets_X)\n",
    "        dY = copy.deepcopy(self.original_datasets_Y)\n",
    "\n",
    "        # Applying transformations\n",
    "        normalize(dX, variables = var)\n",
    "        normalize(dY, variables = [\"TARGETVAR\"])\n",
    "        computeMeanVariance(dX, variables = var, names_m = n_aver,   names_v = n_var, window = window)\n",
    "        computeZonalValue(  dX, variables = var, names_m = n_z_aver, names_v = n_z_var)\n",
    "\n",
    "        # Updating dataset\n",
    "        self.transformed_datasets_X = dX\n",
    "        self.transformed_datasets_Y = dY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0d941d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- DATA LOADER - ILLUSTRATION -- \n",
    "#\n",
    "# Initialization of the loader\n",
    "loader = dataLoader(dataset_original_X, dataset_original_Y)\n",
    "\n",
    "# Aplying the first prototype of pipeline (Mean value on 30 days)\n",
    "loader.pipeline_1(window = 24 * 7)\n",
    "\n",
    "# Check the new shape of the datasets\n",
    "loader.showHeadTransformed()\n",
    "\n",
    "# Combine all the small datasets into a big one\n",
    "loader.combine()\n",
    "\n",
    "# Retreives the train and test set (in Pandas frame)\n",
    "train_X, test_X, train_Y, test_Y = loader.splitTrainTest(numpy = True)\n",
    "\n",
    "# Saving the new dataset\n",
    "loader.save(name = \"Dataset_1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2d570cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "# calculate the correlation matrix\n",
    "corr = train_X.corr()\n",
    "\n",
    "sns.set(rc={'figure.figsize':(25, 20)})\n",
    "\n",
    "# plot the heatmap\n",
    "sns.heatmap(corr, cmap = \"Blues\", annot = True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c53458c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- FIRST MODEL TESTING BUT IT'S LATE I HAVE NO HOPE\n",
    "\n",
    "# Copying df for test\n",
    "dX_train = copy.deepcopy(train_X)\n",
    "dY_train = copy.deepcopy(train_Y)\n",
    "dX_test  = copy.deepcopy(test_X)\n",
    "dY_test  = copy.deepcopy(test_Y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2364426d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 0.925769723088984\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# Initialization of the model\n",
    "model = RandomForestRegressor(max_depth = 20, n_estimators = 50)\n",
    "\n",
    "# Training on the train set\n",
    "model.fit(dX_train, dY_train)\n",
    "\n",
    "# Computing accuracy\n",
    "accuracy = model.score(dX_test, dY_test)\n",
    "\n",
    "print(f\"Accuracy = {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f65278b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "# Initialization of the model\n",
    "model = KNeighborsRegressor(n_neighbors = 10)\n",
    "\n",
    "# Training on the train set\n",
    "model.fit(dX_train_np, dY_train_np)\n",
    "\n",
    "# Computing accuracy\n",
    "accuracy = model.score(dX_test_np, dY_test_np)\n",
    "\n",
    "print(f\"Accuracy = {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d90a2397",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "model = DecisionTreeRegressor(max_depth = 20)\n",
    "\n",
    "# Training on the train set\n",
    "model.fit(dX_train_np, dY_train_np)\n",
    "\n",
    "# Computing accuracy\n",
    "accuracy = model.score(dX_test_np, dY_test_np)\n",
    "\n",
    "print(f\"Accuracy = {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d3cb403",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 64-bit (microsoft store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "bf7aa48c77d89e41b7bdea489bcc8e59001991d3fd115fde96648b6646ee730c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
