{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e8531373-cd83-40d0-bbc2-28b641c208e6",
   "metadata": {},
   "source": [
    "<hr style=\"color:#a4342d;\">\n",
    "<p align=\"center\">\n",
    "    <b style=\"font-size:2.5vw; color:#a4342d; font-weight:bold;\">\n",
    "    Introduction to machine learning - Homework 3\n",
    "    </b>\n",
    "</p>\n",
    "<hr style=\"color:#a4342d;\">\n",
    "\n",
    "<b>Authors</b>: <i>C. Bosch, M. Cornet & V. Mangeleer</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae3b7eac",
   "metadata": {},
   "source": [
    "[comment]: <> (Section)\n",
    "<hr style=\"color:#a4342d;\">\n",
    "<p align=\"center\">\n",
    "    <b style=\"font-size:1.5vw; color:#a4342d;\">\n",
    "    Initialization\n",
    "    </b>\n",
    "</p>\n",
    "<hr style=\"color:#a4342d;\">\n",
    "\n",
    "[comment]: <> (Description)\n",
    "<p align=\"justify\">\n",
    "    In this section, one will be able to initialize all the librairies needed and load the untouched dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae5ae0ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- LIBRAIRIES --\n",
    "import copy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Allow notebook to plot in terminal\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b90a09b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- FUNCTION --\n",
    "\n",
    "# Used to print a basic section title in terminal\n",
    "def section(title = \"UNKNOWN\"):\n",
    "\n",
    "    # Number of letters to determine section size\n",
    "    title_size = len(title)\n",
    "\n",
    "    # Section title boundaries\n",
    "    boundary  = \"-\"\n",
    "    for i in range(title_size + 1):\n",
    "        boundary += \"-\"\n",
    "    \n",
    "    # Printing section\n",
    "    print(boundary)\n",
    "    print(f\" {title} \")\n",
    "    print(boundary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5da1c00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- ORIGINAL DATASET --\n",
    "# The original dataset is contained in the \"data/original\" folder\n",
    "\n",
    "# Stores the original dataset\n",
    "dataset_original_X = []\n",
    "dataset_original_Y = []\n",
    "\n",
    "# Load the original dataset\n",
    "for i in range(1, 11):\n",
    "    dataset_original_X.append(pd.read_csv(f\"data/original/X_Zone_{i}.csv\"))\n",
    "    dataset_original_Y.append(pd.read_csv(f\"data/original/Y_Zone_{i}.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3e0c488",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- BASIC INFORMATION DATASET --\n",
    "\n",
    "# Loading X and Y dataset for the first wind turbine\n",
    "dataset_X1 = dataset_original_X[0]\n",
    "dataset_Y1 = dataset_original_Y[0]\n",
    "\n",
    "# Displaying their relative information\n",
    "section(\"WIND TURBINE 1 - X Dataset\")\n",
    "section(\"HEAD\")\n",
    "print(dataset_X1.head())\n",
    "section(\"INFO\")\n",
    "dataset_X1.info()\n",
    "\n",
    "section(\"WIND TURBINE 1 - Y Dataset\")\n",
    "section(\"HEAD\")\n",
    "print(dataset_Y1.head())\n",
    "section(\"INFO\")\n",
    "dataset_Y1.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd81e84f",
   "metadata": {},
   "source": [
    "[comment]: <> (Section)\n",
    "<hr style=\"color:#a4342d;\">\n",
    "<p align=\"center\">\n",
    "    <b style=\"font-size:1.5vw; color:#a4342d;\">\n",
    "    Exploring\n",
    "    </b>\n",
    "</p>\n",
    "<hr style=\"color:#a4342d;\">\n",
    "\n",
    "[comment]: <> (Description)\n",
    "<p align=\"justify\">\n",
    "    In this section, one will be able to gain some basic insight regarding the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b8fbd5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- HISTOGRAM --\n",
    "\n",
    "# Extracting only relevant variables\n",
    "dataset_X1_relevant = dataset_X1[[\"U10\", \"U100\", \"V10\", \"V100\"]]\n",
    "dataset_Y1_relevant = dataset_Y1[dataset_Y1[\"TARGETVAR\"] >= 0]   # /!\\ Removing test samples (y = -1) /!\\\n",
    "dataset_Y1_relevant = dataset_Y1_relevant[[\"TARGETVAR\"]]\n",
    "\n",
    "# Observing distributions\n",
    "dataset_X1_relevant.hist(bins = 60, figsize = (20, 15))\n",
    "dataset_Y1_relevant.hist(bins = 60, figsize = (15, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef0abd04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- OBSERVING WIND vs POWER --\n",
    "\n",
    "# Removing test data\n",
    "dataset_X_clean = dataset_X1[dataset_Y1[\"TARGETVAR\"] >= 0]\n",
    "dataset_Y_clean = dataset_Y1[dataset_Y1[\"TARGETVAR\"] >= 0]\n",
    "\n",
    "# Computing total wind speed\n",
    "u_wind   = dataset_X_clean[[\"U100\"]].to_numpy()\n",
    "v_wind   = dataset_X_clean[[\"V100\"]].to_numpy()\n",
    "wind_tot = np.sqrt(u_wind**2 + v_wind**2)\n",
    "\n",
    "# Retreiving power\n",
    "power = dataset_Y_clean[[\"TARGETVAR\"]].to_numpy()\n",
    "\n",
    "# To see more clearly, one sample out of 2 is removed\n",
    "for i in range(1):\n",
    "    wind_tot = wind_tot[1::2]\n",
    "    power    = power[1::2]\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(15, 10))\n",
    "plt.scatter(wind_tot, power, s = 3)\n",
    "plt.grid()\n",
    "plt.xlabel(\"Speed [m/s]\")\n",
    "plt.ylabel(\"Normalized Power [-]\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "708c306f",
   "metadata": {},
   "source": [
    "[comment]: <> (Section)\n",
    "<hr style=\"color:#a4342d;\"></hr>\n",
    "<p align=\"center\">\n",
    "    <b style=\"font-size:1.5vw; color:#a4342d;\">\n",
    "    Dataset - Train & Test | DataLoader\n",
    "    </b>\n",
    "</p>\n",
    "<hr style=\"color:#a4342d;\"></hr>\n",
    "\n",
    "[comment]: <> (Description)\n",
    "<p align=\"justify\">\n",
    "    In this section, one will be able to explore further the dataset ! First, one needs to create a train and test set. Then, it is interesitng to look for correlations, new variables and possible improvements to the current dataset. All the new datasets will be save in the datafolder and ready to use by our different models ! The functions available are:\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ae5ae0ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- FUNCTIONS : MEAN, VARIANCE, ZONAL AVERAGE SPEED AND TIME STEPS --\n",
    "#\n",
    "# Used to compute the mean and variance of a variable over some timeslices in the dataset\n",
    "def computeMeanVariance(datasets, \n",
    "                        variables = [\"U100\", \"V100\"],\n",
    "                        window    = 100,\n",
    "                        variance  = True):\n",
    "\n",
    "    # Security\n",
    "    assert window > 1, \"Window size must be greater than 1 to compute mean and var\"\n",
    "\n",
    "    # Looping over all the datasets\n",
    "    for d in datasets:\n",
    "\n",
    "        # Looping over the variables whose mean and var must be computed\n",
    "        for v in variables:\n",
    "\n",
    "            # Retreiving data \n",
    "            data = d.loc[: , [v]].to_numpy()\n",
    "\n",
    "            # Stores mean and variance (1st and 2nd : mean = their value, var = 0 otherwise NAN problem while computation)\n",
    "            mean = [data[0][0], data[1][0]]\n",
    "            var  = [0, 0]\n",
    "\n",
    "            for i in range(2, len(data)):\n",
    "\n",
    "                # Start and end index for computation\n",
    "                index_start = i - window if i - window >= 0 else 0\n",
    "                index_end   = i - 1 if i - 1 >= 0 else 0\n",
    "\n",
    "                # Computing mean and variance (much faster using numpy variables)\n",
    "                mean.append(np.mean(data[index_start:index_end]))\n",
    "                var.append(np.var(data[index_start:index_end]))\n",
    "            \n",
    "            # Adding the new data to dataset\n",
    "            d[f\"{v}_mean\"] = mean\n",
    "            if variance:\n",
    "                d[f\"{v}_var\"] = var\n",
    "\n",
    "# Used to compute the instantenous mean and variance of a variable accross multiple datasets\n",
    "def computeZonalValue(datasets, \n",
    "                      variables = [\"U100\", \"V100\"],\n",
    "                      variance  = True):\n",
    "\n",
    "    # Security\n",
    "    assert len(datasets) > 1, \"To compute mean and var, at least 2 datasets are needed\"\n",
    "\n",
    "    # Looping over the variables whose mean and var must be computed\n",
    "    for v in variables:\n",
    "\n",
    "        # Number of samples\n",
    "        nb_samples = len(datasets[0])\n",
    "\n",
    "        # Stores all the different values in numpy matrix for efficient computation\n",
    "        data = np.zeros((nb_samples, len(datasets)))\n",
    "\n",
    "        # Retreiving all the corresponding data\n",
    "        for i, d in enumerate(datasets):\n",
    "            \n",
    "            # Squeeze is there to remove useless dimension (Ask Victor)\n",
    "            data[:, i] = np.squeeze(d.loc[: , [v]].to_numpy())\n",
    "\n",
    "        # Computing mean and variance (much faster using numpy variables)\n",
    "        mean = np.mean(data, axis = 1) # Axis = 1 to make mean over each row\n",
    "        var  = np.var(data, axis = 1)\n",
    "\n",
    "        # Adding new data to all the datasets\n",
    "        for d in datasets:\n",
    "            d[f\"{v}_mean\"] = mean\n",
    "            if variance:\n",
    "                d[f\"{v}_var\"] = var\n",
    "\n",
    "# Used to add the value taken by a given variable over the past samples\n",
    "def addPastTime(datasets,\n",
    "                variables = [\"U100\", \"V100\"],\n",
    "                window    = 3):\n",
    "    #\n",
    "    # Note from Victor\n",
    "    # This function was a pain in the ass to make ! Even I, am not sure why it works well :D\n",
    "    #\n",
    "    # Security\n",
    "    assert window > 0, \"Window size must be greater than 0 to add past samples\"\n",
    "\n",
    "    # Looping over the datasets\n",
    "    for d in datasets:\n",
    "\n",
    "        # Looping over the different columns\n",
    "        for i, v in enumerate(variables):\n",
    "\n",
    "            # Retrieving current data\n",
    "            data = d[[v]].to_numpy()\n",
    "\n",
    "            # Stores all the past results\n",
    "            former_data = np.zeros((len(data), window))\n",
    "\n",
    "            # Looping over the corresponding data\n",
    "            for j in range(len(data)):\n",
    "\n",
    "                # Start and end index for retreiving values\n",
    "                index_start = j - window if j - window >= 0 else 0\n",
    "                index_end   = j if j - 1 >= 0 else 0\n",
    "                \n",
    "                # Retrieve corresponding value\n",
    "                values = data[index_start:index_end]\n",
    "\n",
    "                # Fixing case where looking at starting indexes < window size\n",
    "                if len(values) != window:\n",
    "                    values = np.append(np.zeros((window - len(values), 1)), values)\n",
    "\n",
    "                # Placing the data (such that by reading left to right: t - 1, t - 2, t - 3, ...)\n",
    "                for k, val in enumerate(values):\n",
    "                        former_data[j][k] = val\n",
    "\n",
    "            # Addding past results in the dataset\n",
    "            for t in range(window):\n",
    "                d[f\"{v}_(t-{window - t})\"] = former_data[:, t]\n",
    "\n",
    "# Used to normalize the data of different variables\n",
    "def normalize(datasets,\n",
    "              norm_type = \"argmax\",\n",
    "              data_type = \"column\",\n",
    "              variables = [\"U100\", \"V100\"]):\n",
    "    \"\"\"\n",
    "    Documentation :\n",
    "        - norm_type (str) : argmax, mean\n",
    "            - Normalize using the argmax or by using the mean and std of the data\n",
    "        - data_type (str) : column, all\n",
    "            - Apply the normalization using norm_type computed either on a unique column or all the same columns\n",
    "    \"\"\"\n",
    "    # Security\n",
    "    assert norm_type in [\"argmax\", \"mean\"], \"Normalization types = argmax, mean\"\n",
    "    assert data_type in [\"column\", \"all\"] , \"Data types = column, all\"\n",
    "\n",
    "    # Initialization of the normalization variables\n",
    "    argmax, mean, std = list(), list(), list()\n",
    "\n",
    "    # 1 - Computing argmax or mean and std of all datasets\n",
    "    if data_type == \"all\":\n",
    "\n",
    "        # Looping over the different variables to normalize\n",
    "        for i, v in enumerate(variables):\n",
    "\n",
    "            # Initialization of the normalization variables\n",
    "            argmax_list, mean_list, std_list = list(), list(), list()\n",
    "\n",
    "            # Looping over all the datasets\n",
    "            for d in datasets:\n",
    "\n",
    "                # Retrieving currently observed data\n",
    "                current_data = d[[v]].to_numpy()\n",
    "\n",
    "                # Retrieving variables\n",
    "                argmax_list.append(np.max(np.abs(current_data)))\n",
    "                mean_list.append(np.mean(current_data))\n",
    "                std_list.append(np.std(current_data))\n",
    "\n",
    "            # Adding results\n",
    "            argmax.append(max(argmax_list))\n",
    "            mean.append(sum(mean_list)/len(mean_list))\n",
    "            std.append(sum(std_list)/len(std_list))\n",
    "    \n",
    "    # 2 - Normalization of the datasets\n",
    "    for d in datasets:\n",
    "\n",
    "        # Looping over the different columns\n",
    "        for i, v in enumerate(variables):\n",
    "            \n",
    "            # Case 1 - Mean and std - Column\n",
    "            if norm_type == \"mean\" and data_type == \"column\":\n",
    "                data         = d[[v]].to_numpy()\n",
    "                d[v] = (data - np.mean(data))/np.std(data)\n",
    "\n",
    "            # Case 2 - Mean and std - All\n",
    "            elif norm_type == \"mean\" and data_type == \"all\":\n",
    "                data         = d[[v]].to_numpy()\n",
    "                d[v] = (data - mean[i])/std[i]\n",
    "            \n",
    "            # Case 3 - Argmax - Column\n",
    "            elif norm_type == \"argmax\" and data_type == \"column\":\n",
    "                data = d[[v]].to_numpy()\n",
    "                d[v] = data/np.max(data)\n",
    "\n",
    "            # Case 4 - Argmax - All\n",
    "            else:\n",
    "                data = d[[v]].to_numpy()\n",
    "                d[v] = data/argmax[i]\n",
    "\n",
    "# Used to remove specific columns from the dataset\n",
    "def remove(datasets, variables):\n",
    "\n",
    "    # Looping over all datasets and variables\n",
    "    for d in datasets:\n",
    "        for v in variables:\n",
    "\n",
    "            # Removing\n",
    "            d.drop(v, inplace = True, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a3e2ef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- DATA LOADER -- \n",
    "# This class has for purpose to handle the data and make our life easier ! \n",
    "#\n",
    "class dataLoader():\n",
    "    \n",
    "    # Initialization of the loader\n",
    "    def __init__(self, datasets_X, datasets_Y):\n",
    "\n",
    "        # Stores the original, transformed and final datasets\n",
    "        self.original_datasets_X    = datasets_X\n",
    "        self.original_datasets_Y    = datasets_Y\n",
    "        self.transformed_datasets_X = datasets_X\n",
    "        self.transformed_datasets_Y = datasets_Y\n",
    "        self.final_dataset_X        = None\n",
    "        self.final_dataset_Y        = None\n",
    "\n",
    "        # Used to know if datasets have been combined or not\n",
    "        self.isCombined = None\n",
    "\n",
    "    # Used to display the head of the transformed dataset (first set)\n",
    "    def showHeadTransformed(self):\n",
    "        section(\"Dataset - X - Transformed\")\n",
    "        print(self.transformed_datasets_X[0].head())\n",
    "        section(\"Dataset - Y - Transformed\")\n",
    "        print(self.transformed_datasets_Y[0].head())\n",
    "\n",
    "    # Used to split the final dataset into a train and test set (In test set, values for y are equal to -1)\n",
    "    def splitTrainTest(self, save = False, save_dir = \"new_data\"):\n",
    "\n",
    "        # Security\n",
    "        assert self.isCombined != None, \"You must first use self.finalize\"\n",
    "\n",
    "        # Case 1 - Datasets have been combined all together\n",
    "        if self.isCombined == True:\n",
    "            X_train = self.final_dataset_X[self.final_dataset_Y['TARGETVAR'] != -1]\n",
    "            Y_train = self.final_dataset_Y[self.final_dataset_Y['TARGETVAR'] != -1]\n",
    "            X_test  = self.final_dataset_X[self.final_dataset_Y['TARGETVAR'] == -1]\n",
    "            Y_test  = self.final_dataset_Y[self.final_dataset_Y['TARGETVAR'] == -1] # Not useful, I know !\n",
    "\n",
    "        # Case 2 - Datasets are still separated\n",
    "        if self.isCombined == False:\n",
    "            \n",
    "            X_train, Y_train, X_test, Y_test = list(), list(), list(), list()\n",
    "\n",
    "            # Looping over all the small datasets\n",
    "            for x, y in zip(self.final_dataset_X, self.final_dataset_Y):\n",
    "                X_train.append(x[y['TARGETVAR'] != -1])\n",
    "                Y_train.append(y[y['TARGETVAR'] != -1])\n",
    "                X_test.append(x[y['TARGETVAR'] == -1])\n",
    "                Y_test.append(y[y['TARGETVAR'] == -1])\n",
    "\n",
    "        # Be careful with the order\n",
    "        return X_train, X_test, Y_train, Y_test\n",
    "        \n",
    "    # Used to perfom final operation on dataset (Combining everything or storing them separately)\n",
    "    def finalization(self, dataset_type = \"combined\"):\n",
    "\n",
    "        # Security\n",
    "        assert dataset_type in [\"combined\", \"separated\"], \"The final dataset can either be of type combined or separated\"\n",
    "\n",
    "        # Case 1 - Combining into one big dataset\n",
    "        if dataset_type == \"combined\":\n",
    "            self.final_dataset_X = pd.concat(self.transformed_datasets_X)\n",
    "            self.final_dataset_Y = pd.concat(self.transformed_datasets_Y)\n",
    "            self.isCombined = True\n",
    "\n",
    "        # Case 2 - Separated datasets\n",
    "        else:\n",
    "            self.final_dataset_X = self.transformed_datasets_X\n",
    "            self.final_dataset_Y = self.transformed_datasets_Y\n",
    "            self.isCombined = False\n",
    "\n",
    "    #--------------------------------------------------------------------------------\n",
    "    #                                    PIPELINES\n",
    "    #--------------------------------------------------------------------------------\n",
    "    #\n",
    "    # List of functions available:\n",
    "    #\n",
    "    # - computeMeanVariance(datasets, variables = ≈, window = 100, variance  = True)\n",
    "    #\n",
    "    # - computeZonalValue(datasets, variables = [\"U100\", \"V100\"], variance  = True)\n",
    "    # \n",
    "    # - addPastTime(datasets, variables = [\"U100\", \"V100\"], window = 3):\n",
    "    #\n",
    "    # - normalize(datasets, norm_type = \"argmax\", data_type = \"column\", variables = [\"U100\", \"V100\"])\n",
    "    #\n",
    "    def pipeline(self, useMeanVariance = True, var_MV   = [\"U10\", \"V10\", \"U100\", \"V100\"], variance_MV  = True, window_MV = 24 * 7,\n",
    "                       useZonal        = True, var_ZON  = [\"U10\", \"V10\", \"U100\", \"V100\"], variance_ZON = True,\n",
    "                       usePastTime     = True, var_PT   = [\"U10\", \"V10\", \"U100\", \"V100\"], window_ZON   = 3,\n",
    "                       useNormalize    = True, var_NORM = [\"U10\", \"V10\", \"U100\", \"V100\"], norm_type = \"argmax\", data_type = \"column\"):\n",
    "\n",
    "        # Copying original dataset\n",
    "        dX = copy.deepcopy(self.original_datasets_X)\n",
    "        dY = copy.deepcopy(self.original_datasets_Y)\n",
    "\n",
    "        # Applying the different transformations\n",
    "        if useNormalize:\n",
    "            normalize(dX, variables = var_NORM, norm_type = norm_type, data_type = data_type)\n",
    "        if useMeanVariance:\n",
    "            computeMeanVariance(dX, variables = var_MV, window = window_MV, variance = variance_MV)\n",
    "        if useZonal:\n",
    "            computeZonalValue(dX, variables = var_ZON, variance = variance_ZON)\n",
    "        if usePastTime:\n",
    "            addPastTime(dX, variables = var_PT, window = window_ZON)\n",
    "\n",
    "        # Updating dataset\n",
    "        self.transformed_datasets_X = dX\n",
    "        self.transformed_datasets_Y = dY\n",
    "\n",
    "        # Making sure one has to finalize again\n",
    "        self.isCombined = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "521edde9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- GAINING INSIGHTS (2) - ARGMAX ALL -- \n",
    "#\n",
    "# Treshold value for removing correlation\n",
    "tresh_corr = 0.3\n",
    "\n",
    "# Initialization of the loader\n",
    "loader_2 = dataLoader(dataset_original_X, dataset_original_Y)\n",
    "\n",
    "# Aplying the first prototype of pipeline (Mean value on 30 days)\n",
    "loader_2.pipeline(norm_type = \"argmax\", data_type = \"all\")\n",
    "\n",
    "# Combine all the small datasets into a big one\n",
    "loader_2.finalization(dataset_type = \"combined\")\n",
    "\n",
    "# Retreives the train and test set (in Pandas frame)\n",
    "data_X_2, _, _, _ = loader_2.splitTrainTest()\n",
    "\n",
    "# -- Correlation matrix -- \n",
    "corr_2             = data_X_2.corr()\n",
    "corr_2[np.abs(corr_2) < tresh_corr] = 0\n",
    "sns.set(rc={'figure.figsize':(25, 20)})\n",
    "sns.heatmap(corr_2, cmap = \"YlGnBu\", annot = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "418e42dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- GAINING INSIGHTS (3) - MEAN COLUMN -- \n",
    "#\n",
    "# Treshold value for removing correlation\n",
    "tresh_corr = 0.3\n",
    "\n",
    "# Initialization of the loader\n",
    "loader_3 = dataLoader(dataset_original_X, dataset_original_Y)\n",
    "\n",
    "# Aplying the first prototype of pipeline (Mean value on 30 days)\n",
    "loader_3.pipeline(norm_type = \"mean\", data_type = \"column\")\n",
    "\n",
    "# Combine all the small datasets into a big one\n",
    "loader_3.finalization(dataset_type = \"combined\")\n",
    "\n",
    "# Retreives the train and test set (in Pandas frame)\n",
    "data_X_3, _, _, _ = loader_3.splitTrainTest()\n",
    "\n",
    "# -- Correlation matrix -- \n",
    "corr_3             = data_X_3.corr()\n",
    "corr_3[np.abs(corr_3) < tresh_corr] = 0\n",
    "sns.set(rc={'figure.figsize':(25, 20)})\n",
    "sns.heatmap(corr_3, cmap = \"BuPu\", annot = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7066f271",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- GAINING INSIGHTS (4) - MEAN ALL -- \n",
    "#\n",
    "# Treshold value for removing correlation\n",
    "tresh_corr = 0.3\n",
    "\n",
    "# Initialization of the loader\n",
    "loader_4 = dataLoader(dataset_original_X, dataset_original_Y)\n",
    "\n",
    "# Aplying the first prototype of pipeline (Mean value on 30 days)\n",
    "loader_4.pipeline(norm_type = \"mean\", data_type = \"all\")\n",
    "\n",
    "# Combine all the small datasets into a big one\n",
    "loader_4.finalization(dataset_type = \"combined\")\n",
    "\n",
    "# Retreives the train and test set (in Pandas frame)\n",
    "data_X_4, _, _, _ = loader_4.splitTrainTest()\n",
    "\n",
    "# -- Correlation matrix -- \n",
    "corr_4             = data_X_4.corr()\n",
    "corr_4[np.abs(corr_4) < tresh_corr] = 0\n",
    "sns.set(rc={'figure.figsize':(25, 20)})\n",
    "sns.heatmap(corr_4, cmap = \"Greens\", annot = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad5741b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- CORRELATION MATRIX COMPARISON --\n",
    "corr_21 = corr_2 - corr_1\n",
    "sns.set(rc={'figure.figsize':(25, 20)})\n",
    "sns.heatmap(corr_21, cmap = \"Greens\", annot = True)\n",
    "\n",
    "plt.figure()\n",
    "corr_23 = corr_2 - corr_3\n",
    "sns.set(rc={'figure.figsize':(25, 20)})\n",
    "sns.heatmap(corr_23, cmap = \"BuPu\", annot = True)\n",
    "\n",
    "plt.figure()\n",
    "corr_24 = corr_2 - corr_4\n",
    "sns.set(rc={'figure.figsize':(25, 20)})\n",
    "sns.heatmap(corr_24, cmap = \"YlGnBu\", annot = True)\n",
    "\n",
    "plt.figure()\n",
    "corr_31 = corr_3 - corr_1\n",
    "corr_31[np.abs(corr_31) < 0.2] = 0\n",
    "sns.set(rc={'figure.figsize':(25, 20)})\n",
    "sns.heatmap(corr_31, cmap = \"BuPu\", annot = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c7ea381",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Template\n",
    "loader.pipeline(useMeanVariance = True, var_MV   = [\"U10\", \"V10\", \"U100\", \"V100\"], variance_MV  = True, window_MV = 24 * 7,\n",
    "                useZonal        = True, var_ZON  = [\"U10\", \"V10\", \"U100\", \"V100\"], variance_ZON = True,\n",
    "                usePastTime     = True, var_PT   = [\"U10\", \"V10\", \"U100\", \"V100\"], window_ZON = 3,\n",
    "                useNormalize    = True, var_NORM = [\"U10\", \"V10\", \"U100\", \"V100\"], norm_type = \"argmax\", data_type = \"column\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "728a4aa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialization of the loader\n",
    "loader = dataLoader(dataset_original_X, dataset_original_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fda4aa4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TYPE 1 - ORIGINAL\n",
    "loader.pipeline(useMeanVariance = False,\n",
    "                useZonal        = False,\n",
    "                usePastTime     = False,\n",
    "                useNormalize    = False)\n",
    "loader.finalization()\n",
    "\n",
    "data_X0, submit_X0, data_Y0, submit_Y0 = loader.splitTrainTest()\n",
    "print(data_X0.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c4fca71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TYPE 2 - INFLUENCE OF NORMALIZATION\n",
    "loader.pipeline(useMeanVariance = False,\n",
    "                useZonal        = False,\n",
    "                usePastTime     = False,\n",
    "                useNormalize    = True, norm_type = \"argmax\", data_type = \"column\")\n",
    "                \n",
    "loader.finalization()\n",
    "data_X1, submit_X1, data_Y1, submit_Y1 = loader.splitTrainTest()\n",
    "print(data_X1.head())\n",
    "\n",
    "loader.pipeline(useMeanVariance = False,\n",
    "                useZonal        = False,\n",
    "                usePastTime     = False,\n",
    "                useNormalize    = True, norm_type = \"argmax\", data_type = \"all\")\n",
    "                \n",
    "loader.finalization()\n",
    "data_X2, submit_X2, data_Y2, submit_Y2 = loader.splitTrainTest()\n",
    "print(data_X2.head())\n",
    "\n",
    "loader.pipeline(useMeanVariance = False,\n",
    "                useZonal        = False,\n",
    "                usePastTime     = False,\n",
    "                useNormalize    = True, norm_type = \"mean\", data_type = \"column\")\n",
    "                \n",
    "loader.finalization()\n",
    "data_X3, submit_X3, data_Y3, submit_Y3 = loader.splitTrainTest()\n",
    "print(data_X3.head())\n",
    "\n",
    "loader.pipeline(useMeanVariance = False,\n",
    "                useZonal        = False,\n",
    "                usePastTime     = False,\n",
    "                useNormalize    = True, norm_type = \"mean\", data_type = \"all\")\n",
    "                \n",
    "loader.finalization()\n",
    "data_X4, submit_X4, data_Y4, submit_Y4 = loader.splitTrainTest()\n",
    "print(data_X4.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "201a1e56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- Datasets --\n",
    "data_X_tot = [data_X0, data_X1, data_X2, data_X3, data_X4]\n",
    "data_Y_tot = [data_Y0, data_Y1, data_Y2, data_Y3, data_Y4]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6b1cb5f",
   "metadata": {},
   "source": [
    "[comment]: <> (Section)\n",
    "<hr style=\"color:#a4342d;\"></hr>\n",
    "<p align=\"center\">\n",
    "    <b style=\"font-size:1.5vw; color:#a4342d;\">\n",
    "    Model - Training & Testing\n",
    "    </b>\n",
    "</p>\n",
    "<hr style=\"color:#a4342d;\"></hr>\n",
    "\n",
    "[comment]: <> (Description)\n",
    "<p align=\"justify\">\n",
    "    In this section, one will be able to explore further the dataset ! First, one needs to create a train and test set. Then, it is interesitng to look for correlations, new variables and possible improvements to the current dataset. All the new datasets will be save in the datafolder and ready to use by our different models ! The functions available are:\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4239fcaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- FUNCTIONS --\n",
    "#\n",
    "# Used to compute a model's accuracy against different datasets\n",
    "def modelTesting(datasets_X, datasets_y, model, test_size = 0.3, random_state = 69):\n",
    "    \n",
    "    # Contains mean accuracy of the model against each dataset\n",
    "    accuracy_train = []\n",
    "    accuracy_test = []\n",
    "\n",
    "    # Looping over whole the different datasets\n",
    "    for X, y in zip(datasets_X, datasets_y):\n",
    "        \n",
    "        # Final conversion (Numpy and retrieving targets)\n",
    "        X = X.to_numpy()\n",
    "        y = y[[\"TARGETVAR\"]].to_numpy().ravel()\n",
    "\n",
    "        # Retrieving datasets\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = test_size, random_state = random_state)\n",
    "\n",
    "        # Fitting the model on current split\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        # Accuracy\n",
    "        accuracy_train.append(model.score(X_train, y_train))\n",
    "        accuracy_test.append(model.score(X_test, y_test))\n",
    "\n",
    "    return accuracy_train, accuracy_test\n",
    "\n",
    "def modelPlotResults(parameters, acc_train, acc_test, xlabel = \"UNKNOWN\", param_name = \"UNKNOWN\", fontsize = 15, save_path = \"graphs/\"):\n",
    "\n",
    "    # 1 - Evolution of the test accuracy\n",
    "    plt.figure()\n",
    "    sns.set(font_scale = 2)\n",
    "\n",
    "    # Plotting evolution curve for a specific dataset with varying parameter value\n",
    "    for i, a in enumerate(acc_test):\n",
    "        plt.plot(parameters, [a_i * 100 for a_i in a], label = f\"Dataset n°{i}\", linewidth = 5)\n",
    "    \n",
    "    plt.legend(loc=\"upper right\", fontsize = fontsize)\n",
    "    plt.ylabel(\"Accuracy [%]\", fontsize = fontsize)\n",
    "    plt.xlabel(xlabel, fontsize = fontsize)\n",
    "    plt.savefig(f\"{save_path}_1.png\")\n",
    "    plt.show()\n",
    "\n",
    "    # 2 - Bar plot\n",
    "    acc_test_best  = []\n",
    "    acc_train_best = []\n",
    "    k_best         = []\n",
    "\n",
    "    # Looping over accuracies to find best results\n",
    "    for a1, a2 in zip(acc_train, acc_test):\n",
    "\n",
    "        # Finding best test accuracy\n",
    "        max_value = max(a2)\n",
    "        max_index = a2.index(max_value)\n",
    "\n",
    "        # Adding results\n",
    "        acc_test_best.append(max_value)\n",
    "        acc_train_best.append(a1[max_index])\n",
    "        k_best.append(parameters[max_index])\n",
    "\n",
    "    # Contains x-axis labels\n",
    "    x_ax_labels = [f\"Dataset n°{i} - {param_name} = {k_best[i]}\" for i in range(len(acc_train))]\n",
    "\n",
    "    # Used to make x-axis    \n",
    "    index = [i for i in range(len(acc_train))]\n",
    "\n",
    "    # Plotting the results\n",
    "    plt.figure()\n",
    "    plt.bar([i - 0.2 for i in index], [a_i * 100 for a_i in acc_train_best], 0.4, label = \"Train\")\n",
    "    plt.bar([i + 0.2 for i in index], [a_i * 100 for a_i in acc_test_best], 0.4, label = \"Test\")\n",
    "    plt.xticks(index, x_ax_labels)\n",
    "    plt.ylabel(\"Accuracy [%]\", fontsize = fontsize)\n",
    "    plt.legend()\n",
    "    plt.savefig(f\"{save_path}_2.png\")\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc10f146",
   "metadata": {},
   "source": [
    "<hr style=\"color:#a4342d; width: 145px;\" align=\"left\">\n",
    "<p style=\"color:#a4342d;\">KNeighborsRegressor</p>\n",
    "<hr style=\"color:#a4342d; width: 145px;\" align=\"left\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "f902f469",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "n_neighbors does not take <class 'numpy.float64'> value, enter integer value",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/vikvador/Documents/iml/IML-Homework3/IML_HW3.ipynb Cellule 25\u001b[0m in \u001b[0;36m<cell line: 11>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/vikvador/Documents/iml/IML-Homework3/IML_HW3.ipynb#X41sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m model \u001b[39m=\u001b[39m KNeighborsRegressor(n_neighbors \u001b[39m=\u001b[39m k)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/vikvador/Documents/iml/IML-Homework3/IML_HW3.ipynb#X41sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m \u001b[39m# Computing accuracies on all the datasets\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/vikvador/Documents/iml/IML-Homework3/IML_HW3.ipynb#X41sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m acc_train, acc_test \u001b[39m=\u001b[39m modelTesting(data_X_tot, data_Y_tot, model, test_size \u001b[39m=\u001b[39;49m \u001b[39m0.3\u001b[39;49m, random_state \u001b[39m=\u001b[39;49m \u001b[39m69\u001b[39;49m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/vikvador/Documents/iml/IML-Homework3/IML_HW3.ipynb#X41sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m \u001b[39m# Adding the results\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/vikvador/Documents/iml/IML-Homework3/IML_HW3.ipynb#X41sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m \u001b[39mfor\u001b[39;00m i, acc_1, acc_2 \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(\u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(acc_train)), acc_train, acc_test):\n",
      "\u001b[1;32m/Users/vikvador/Documents/iml/IML-Homework3/IML_HW3.ipynb Cellule 25\u001b[0m in \u001b[0;36mmodelTesting\u001b[0;34m(datasets_X, datasets_y, model, test_size, random_state)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/vikvador/Documents/iml/IML-Homework3/IML_HW3.ipynb#X41sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m X_train, X_test, y_train, y_test \u001b[39m=\u001b[39m train_test_split(X, y, test_size \u001b[39m=\u001b[39m test_size, random_state \u001b[39m=\u001b[39m random_state)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/vikvador/Documents/iml/IML-Homework3/IML_HW3.ipynb#X41sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m \u001b[39m# Fitting the model on current split\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/vikvador/Documents/iml/IML-Homework3/IML_HW3.ipynb#X41sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m model\u001b[39m.\u001b[39;49mfit(X_train, y_train)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/vikvador/Documents/iml/IML-Homework3/IML_HW3.ipynb#X41sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m \u001b[39m# Accuracy\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/vikvador/Documents/iml/IML-Homework3/IML_HW3.ipynb#X41sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m accuracy_train\u001b[39m.\u001b[39mappend(model\u001b[39m.\u001b[39mscore(X_train, y_train))\n",
      "File \u001b[0;32m~/opt/miniconda3/lib/python3.9/site-packages/sklearn/neighbors/_regression.py:213\u001b[0m, in \u001b[0;36mKNeighborsRegressor.fit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    194\u001b[0m \u001b[39m\"\"\"Fit the k-nearest neighbors regressor from the training dataset.\u001b[39;00m\n\u001b[1;32m    195\u001b[0m \n\u001b[1;32m    196\u001b[0m \u001b[39mParameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    209\u001b[0m \u001b[39m    The fitted k-nearest neighbors regressor.\u001b[39;00m\n\u001b[1;32m    210\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    211\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mweights \u001b[39m=\u001b[39m _check_weights(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mweights)\n\u001b[0;32m--> 213\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit(X, y)\n",
      "File \u001b[0;32m~/opt/miniconda3/lib/python3.9/site-packages/sklearn/neighbors/_base.py:571\u001b[0m, in \u001b[0;36mNeighborsBase._fit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    569\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mExpected n_neighbors > 0. Got \u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_neighbors)\n\u001b[1;32m    570\u001b[0m     \u001b[39melif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_neighbors, numbers\u001b[39m.\u001b[39mIntegral):\n\u001b[0;32m--> 571\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\n\u001b[1;32m    572\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mn_neighbors does not take \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m value, enter integer value\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    573\u001b[0m             \u001b[39m%\u001b[39m \u001b[39mtype\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_neighbors)\n\u001b[1;32m    574\u001b[0m         )\n\u001b[1;32m    576\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n",
      "\u001b[0;31mTypeError\u001b[0m: n_neighbors does not take <class 'numpy.float64'> value, enter integer value"
     ]
    }
   ],
   "source": [
    "# -- Generating results KNN -- \n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "# Definition of the parameters to be tested\n",
    "k_param = np.linspace(1, 100, 100, dtype = int)\n",
    "\n",
    "# Stores the accuracy of the training and testing\n",
    "knn_accuracy_train = [[] for i in range(len(data_X_tot))]\n",
    "knn_accuracy_test  = [[] for i in range(len(data_X_tot))]\n",
    "\n",
    "for k in k_param:\n",
    "\n",
    "    # Initialization of the model\n",
    "    model = KNeighborsRegressor(n_neighbors = k)\n",
    "\n",
    "    # Computing accuracies on all the datasets\n",
    "    acc_train, acc_test = modelTesting(data_X_tot, data_Y_tot, model, test_size = 0.3, random_state = 69)\n",
    "\n",
    "    # Adding the results\n",
    "    for i, acc_1, acc_2 in zip(range(len(acc_train)), acc_train, acc_test):\n",
    "        knn_accuracy_train[i].append(acc_1)\n",
    "        knn_accuracy_test[i].append(acc_2)\n",
    "\n",
    "# Plotting the results\n",
    "modelPlotResults(k_param, knn_accuracy_train, knn_accuracy_test, \n",
    "                 xlabel = \"Number of neighbors - $k$ [-]\", fontsize = 30, \n",
    "                 param_name = \"knn\", save_path = \"graphs/knn/knn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dc245e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "modelPlotResults(k_param, knn_accuracy_train, knn_accuracy_test, \n",
    "                 xlabel = \"Number of neighbors - $k$ [-]\", fontsize = 30, \n",
    "                 param_name = \"k\", save_path = \"graphs/knn/knn\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ca89775",
   "metadata": {},
   "source": [
    "<hr style=\"color:#a4342d; width: 100px;\" align=\"left\">\n",
    "<p style=\"color:#a4342d;\">Random Forest</p>\n",
    "<hr style=\"color:#a4342d; width: 100px;\" align=\"left\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4f1a619",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- Generating results Random Forest -- \n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# Definition of the parameters to be tested\n",
    "max_depth = np.linspace(1, 40, 40)\n",
    "\n",
    "# Stores the accuracy of the training and testing\n",
    "rf_accuracy_train = [[] for i in range(len(data_X_tot))]\n",
    "rf_accuracy_test  = [[] for i in range(len(data_X_tot))]\n",
    "\n",
    "for d in max_depth:\n",
    "\n",
    "    # Initialization of the model\n",
    "    model = RandomForestRegressor(max_depth = d, n_estimators = 10)\n",
    "\n",
    "    # Computing accuracies on all the datasets\n",
    "    acc_train, acc_test = modelTesting(data_X_tot, data_Y_tot, model, test_size = 0.3, random_state = 69)\n",
    "\n",
    "    # Adding the results\n",
    "    for i, acc_1, acc_2 in zip(range(len(acc_train)), acc_train, acc_test):\n",
    "        rf_accuracy_train[i].append(acc_1)\n",
    "        rf_accuracy_test[i].append(acc_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "390066d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the results\n",
    "modelPlotResults(max_depth, rf_accuracy_train, rf_accuracy_test, \n",
    "                 xlabel = \"Depth of the tree - $d$ [-]\", fontsize = 30, \n",
    "                 param_name = \"d\", save_path = \"graphs/rf/rf\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 64-bit (microsoft store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "bf7aa48c77d89e41b7bdea489bcc8e59001991d3fd115fde96648b6646ee730c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
