{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e8531373-cd83-40d0-bbc2-28b641c208e6",
   "metadata": {},
   "source": [
    "<hr style=\"color:#a4342d;\">\n",
    "<p align=\"center\">\n",
    "    <b style=\"font-size:2.5vw; color:#a4342d; font-weight:bold;\">\n",
    "    Introduction to machine learning - Homework 3\n",
    "    </b>\n",
    "</p>\n",
    "<hr style=\"color:#a4342d;\">\n",
    "\n",
    "<b>Authors</b>: <i>C. Bosch, M. Cornet & V. Mangeleer</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae3b7eac",
   "metadata": {},
   "source": [
    "[comment]: <> (Section)\n",
    "<hr style=\"color:#a4342d;\">\n",
    "<p align=\"center\">\n",
    "    <b style=\"font-size:1.5vw; color:#a4342d;\">\n",
    "    Initialization\n",
    "    </b>\n",
    "</p>\n",
    "<hr style=\"color:#a4342d;\">\n",
    "\n",
    "[comment]: <> (Description)\n",
    "<p align=\"justify\">\n",
    "    In this section, one will be able to initialize all the librairies needed and load the untouched dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- LIBRAIRIES --\n",
    "import copy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MaxAbsScaler, RobustScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from statsmodels.graphics.tsaplots import plot_pacf, plot_acf\n",
    "\n",
    "from scipy.fft import fft, fftfreq\n",
    "\n",
    "# Allow notebook to plot in terminal\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- FUNCTION --\n",
    "\n",
    "# Used to print a basic section title in terminal\n",
    "def section(title = \"UNKNOWN\"):\n",
    "\n",
    "    # Number of letters to determine section size\n",
    "    title_size = len(title)\n",
    "\n",
    "    # Section title boundaries\n",
    "    boundary  = \"-\"\n",
    "    for i in range(title_size + 1):\n",
    "        boundary += \"-\"\n",
    "    \n",
    "    # Printing section\n",
    "    print(boundary)\n",
    "    print(f\" {title} \")\n",
    "    print(boundary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- ORIGINAL DATASET --\n",
    "# The original dataset is contained in the \"data/original\" folder\n",
    "\n",
    "# Stores the original dataset\n",
    "dataset_original_X = []\n",
    "dataset_original_Y = []\n",
    "\n",
    "# Load the original dataset\n",
    "for i in range(1, 11):\n",
    "    dataset_original_X.append(pd.read_csv(f\"data/original/X_Zone_{i}.csv\"))\n",
    "    dataset_original_Y.append(pd.read_csv(f\"data/original/Y_Zone_{i}.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- BASIC INFORMATION DATASET --\n",
    "\n",
    "# Loading X and Y dataset for the first wind turbine\n",
    "dataset_X1 = dataset_original_X[0]\n",
    "dataset_Y1 = dataset_original_Y[0]\n",
    "\n",
    "# Displaying their relative information\n",
    "section(\"WIND TURBINE 1 - X Dataset\")\n",
    "section(\"HEAD\")\n",
    "print(dataset_X1.head())\n",
    "section(\"INFO\")\n",
    "dataset_X1.info()\n",
    "\n",
    "section(\"WIND TURBINE 1 - Y Dataset\")\n",
    "section(\"HEAD\")\n",
    "print(dataset_Y1.head())\n",
    "section(\"INFO\")\n",
    "dataset_Y1.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "708c306f",
   "metadata": {},
   "source": [
    "[comment]: <> (Section)\n",
    "<hr style=\"color:#a4342d;\"></hr>\n",
    "<p align=\"center\">\n",
    "    <b style=\"font-size:1.5vw; color:#a4342d;\">\n",
    "    Dataset - Train & Test | DataLoader\n",
    "    </b>\n",
    "</p>\n",
    "<hr style=\"color:#a4342d;\"></hr>\n",
    "\n",
    "[comment]: <> (Description)\n",
    "<p align=\"justify\">\n",
    "    In this section, one will be able to explore further the dataset ! First, one needs to create a train and test set. Then, it is interestng to look for correlations, new variables and possible improvements to the current dataset. All the new datasets will be save in the datafolder and ready to use by our different models ! The functions available are:\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- FUNCTIONS : MEAN, VARIANCE, ZONAL AVERAGE SPEED AND TIME STEPS --\n",
    "\n",
    "# Used to compute the mean and variance of a variable over some timeslices in the dataset\n",
    "def computeMeanVariance(datasets, \n",
    "                        variables = [\"U100\", \"V100\"],\n",
    "                        window    = 100,\n",
    "                        variance  = True):\n",
    "\n",
    "    # Security\n",
    "    assert window > 1, \"Window size must be greater than 1 to compute mean and var\"\n",
    "\n",
    "    # Looping over all the datasets\n",
    "    for d in datasets:\n",
    "\n",
    "        # Looping over the variables whose mean and var must be computed\n",
    "        for v in variables:\n",
    "\n",
    "            # Retreiving data \n",
    "            data = d.loc[: , [v]].to_numpy()\n",
    "\n",
    "            # Stores mean and variance (1st and 2nd : mean = their value, var = 0 otherwise NAN problem while computation)\n",
    "            mean = [data[0][0], data[1][0]]\n",
    "            var  = [0, 0]\n",
    "\n",
    "            for i in range(2, len(data)):\n",
    "\n",
    "                # Start and end index for computation\n",
    "                index_start = i - window if i - window >= 0 else 0\n",
    "                index_end   = i - 1 if i - 1 >= 0 else 0\n",
    "\n",
    "                # Computing mean and variance (much faster using numpy variables)\n",
    "                mean.append(np.mean(data[index_start:index_end]))\n",
    "                var.append(np.var(data[index_start:index_end]))\n",
    "            \n",
    "            # Adding the new data to dataset\n",
    "            d[f\"{v}_mean\"] = mean\n",
    "            if variance:\n",
    "                d[f\"{v}_var\"] = var\n",
    "\n",
    "# Used to compute the instantenous mean and variance of a variable accross multiple datasets\n",
    "def computeZonalValue(datasets, \n",
    "                      variables = [\"U100\", \"V100\"],\n",
    "                      variance  = True):\n",
    "\n",
    "    # Security\n",
    "    assert len(datasets) > 1, \"To compute mean and var, at least 2 datasets are needed\"\n",
    "\n",
    "    # Looping over the variables whose mean and var must be computed\n",
    "    for v in variables:\n",
    "\n",
    "        # Number of samples\n",
    "        nb_samples = len(datasets[0])\n",
    "\n",
    "        # Stores all the different values in numpy matrix for efficient computation\n",
    "        data = np.zeros((nb_samples, len(datasets)))\n",
    "\n",
    "        # Retreiving all the corresponding data\n",
    "        for i, d in enumerate(datasets):\n",
    "            \n",
    "            # Squeeze is there to remove useless dimension (Ask Victor)\n",
    "            data[:, i] = np.squeeze(d.loc[: , [v]].to_numpy())\n",
    "\n",
    "        # Computing mean and variance (much faster using numpy variables)\n",
    "        mean = np.mean(data, axis = 1) # Axis = 1 to make mean over each row\n",
    "        var  = np.var(data, axis = 1)\n",
    "\n",
    "        # Adding new data to all the datasets\n",
    "        for d in datasets:\n",
    "            d[f\"{v}_mean\"] = mean\n",
    "            if variance:\n",
    "                d[f\"{v}_var\"] = var\n",
    "\n",
    "# Used to add the value taken by a given variable over the past samples\n",
    "def addPastTime(datasets,\n",
    "                variables = [\"U100\", \"V100\"],\n",
    "                window    = 3):\n",
    "    #\n",
    "    # Note from Victor\n",
    "    # This function was a pain in the ass to make ! Even I, am not sure why it works well :D\n",
    "    #\n",
    "    # Security\n",
    "    assert window > 0, \"Window size must be greater than 0 to add past samples\"\n",
    "\n",
    "    # Looping over the datasets\n",
    "    for d in datasets:\n",
    "\n",
    "        # Looping over the different columns\n",
    "        for i, v in enumerate(variables):\n",
    "\n",
    "            # Retrieving current data\n",
    "            data = d[[v]].to_numpy()\n",
    "\n",
    "            # Stores all the past results\n",
    "            former_data = np.zeros((len(data), window))\n",
    "\n",
    "            # Looping over the corresponding data\n",
    "            for j in range(len(data)):\n",
    "\n",
    "                # Start and end index for retreiving values\n",
    "                index_start = j - window if j - window >= 0 else 0\n",
    "                index_end   = j if j - 1 >= 0 else 0\n",
    "                \n",
    "                # Retrieve corresponding value\n",
    "                values = data[index_start:index_end]\n",
    "\n",
    "                # Fixing case where looking at starting indexes < window size\n",
    "                if len(values) != window:\n",
    "                    values = np.append(np.zeros((window - len(values), 1)), values) # here attention\n",
    "\n",
    "                # Placing the data (such that by reading left to right: t - 1, t - 2, t - 3, ...)\n",
    "                for k, val in enumerate(values):\n",
    "                        former_data[j][k] = val\n",
    "\n",
    "            # Addding past results in the dataset\n",
    "            for t in range(window):\n",
    "                d[f\"{v}_(t-{window - t})\"] = former_data[:, t]\n",
    "\n",
    "# Used to normalize the data of different variables\n",
    "def normalize(datasets,\n",
    "              norm_type = \"argmax\",\n",
    "              data_type = \"column\",\n",
    "              variables = [\"U10\", \"V10\", \"U100\", \"V100\"]):\n",
    "    \"\"\"\n",
    "    Documentation :\n",
    "        - norm_type (str) : argmax, mean, WSmax, WSmean\n",
    "            - Normalize using the argmax or by using the mean and std of the data\n",
    "        - data_type (str) : column, all\n",
    "            - Apply the normalization using norm_type computed either on a unique column or all the same columns\n",
    "    \"\"\"\n",
    "    # Security\n",
    "    assert norm_type in [\"argmax\", \"mean\"], \"Normalization types = argmax, mean\"\n",
    "    assert data_type in [\"column\", \"all\"] , \"Data types = column, all\"\n",
    "\n",
    "    # Initialization of the normalization variables\n",
    "    argmax, mean, std = list(), list(), list()\n",
    "\n",
    "    # 1 - Computing argmax or mean and std of all datasets\n",
    "    if data_type == \"all\":\n",
    "\n",
    "        # Looping over the different variables to normalize\n",
    "        for i, v in enumerate(variables):\n",
    "\n",
    "            # Initialization of the normalization variables\n",
    "            argmax_list, mean_list, std_list = list(), list(), list()\n",
    "\n",
    "            # Looping over all the datasets\n",
    "            for d in datasets:\n",
    "\n",
    "                # Retrieving currently observed data\n",
    "                current_data = d[[v]].to_numpy()\n",
    "\n",
    "                # Retrieving variables\n",
    "                argmax_list.append(np.max(np.abs(current_data)))\n",
    "                mean_list.append(np.mean(current_data))\n",
    "                std_list.append(np.std(current_data))\n",
    "\n",
    "                \n",
    "\n",
    "            # Adding results\n",
    "            argmax.append(max(argmax_list))\n",
    "            mean.append(sum(mean_list)/len(mean_list))\n",
    "            std.append(sum(std_list)/len(std_list))\n",
    "    \n",
    "    # 2 - Normalization of the datasets\n",
    "    for d in datasets:\n",
    "\n",
    "        # Looping over the different columns\n",
    "        for i, v in enumerate(variables):\n",
    "            \n",
    "            # Case 1 - Mean and std - Column\n",
    "            if norm_type == \"mean\" and data_type == \"column\":\n",
    "                data         = d[[v]].to_numpy()\n",
    "                d[v] = (data - np.mean(data))/np.std(data)\n",
    "\n",
    "            # Case 2 - Mean and std - All\n",
    "            elif norm_type == \"mean\" and data_type == \"all\":\n",
    "                data         = d[[v]].to_numpy()\n",
    "                d[v] = (data - mean[i])/std[i]\n",
    "            \n",
    "            # Case 3 - Argmax - Column\n",
    "            elif norm_type == \"argmax\" and data_type == \"column\":\n",
    "                data = d[[v]].to_numpy()\n",
    "                d[v] = data/np.max(data)\n",
    "\n",
    "            # Case 4 - Argmax - All\n",
    "            else:\n",
    "                data = d[[v]].to_numpy()\n",
    "                d[v] = data/argmax[i]\n",
    "    #return datasets\n",
    "\n",
    "# Used to remove specific columns from the dataset\n",
    "def remove(datasets, variables):\n",
    "\n",
    "    # Looping over all datasets and variables\n",
    "    for d in datasets:\n",
    "        for v in variables:\n",
    "\n",
    "            # Removing\n",
    "            d.drop(v, inplace = True, axis = 1)\n",
    "\n",
    "def normalize_bis(datasets,\n",
    "                  norm_type = \"max_abs\",\n",
    "                  data_type = \"column\",\n",
    "                  variables = [\"U10\", \"V10\", \"U100\", \"V100\"]):\n",
    "\n",
    "    \n",
    "    if data_type == \"column\":\n",
    "        for d in datasets:\n",
    "            data = d[variables].to_numpy()\n",
    "            if norm_type == \"max_abs\":\n",
    "                max_abs_scaler = MaxAbsScaler()\n",
    "                scaled_features = max_abs_scaler.fit_transform(data)\n",
    "                d[variables] = scaled_features\n",
    "            elif norm_type == \"standard\":\n",
    "                std_scaler = StandardScaler()\n",
    "                scaled_features = std_scaler.fit_transform(data)\n",
    "                d[variables] = scaled_features\n",
    "            elif norm_type == \"robust\":\n",
    "                robust_scaler = RobustScaler()\n",
    "                scaled_features = robust_scaler.fit_transform(data)\n",
    "                d[variables] = scaled_features\n",
    "\n",
    "        \n",
    "    elif data_type == \"all\":\n",
    "\n",
    "        data = pd.concat(datasets, keys=[i for i in range(len(datasets))])\n",
    "        data_np = data[variables].to_numpy()\n",
    "        if norm_type == \"max_abs\":\n",
    "            max_abs_scaler = MaxAbsScaler()\n",
    "            scaled_features = max_abs_scaler.fit_transform(data_np)\n",
    "            data[variables] = scaled_features\n",
    "        elif norm_type == \"standard\":\n",
    "            std_scaler = StandardScaler()\n",
    "            scaled_features = std_scaler.fit_transform(data_np)\n",
    "            data[variables] = scaled_features\n",
    "        elif norm_type == \"robust\":\n",
    "            robust_scaler = RobustScaler()\n",
    "            scaled_features = robust_scaler.fit_transform(data_np)\n",
    "            data[variables] = scaled_features\n",
    "\n",
    "        for i in range(len(datasets)):\n",
    "            datasets[i] = data.loc[i, :]\n",
    "    \n",
    "\n",
    "# Used to retrieve the wind direction and the wind speed from\n",
    "# meridional and zonal comp and add them in the dataset\n",
    "def wind_comp(datasets, columns=\"both\", speed_height=\"both\"):\n",
    "    # columns must be equal to \"wind_speed\", \"wind_direction\", \"both\"\n",
    "  \n",
    "    # Find the speed speed and the wind direction\n",
    "    # speed height must be 10 or 100 or both\n",
    "    for d in datasets:\n",
    "        if speed_height == \"10\" or speed_height == \"both\":\n",
    "            w_u_10 = d[\"U10\"].to_numpy()\n",
    "            w_v_10 = d[\"V10\"].to_numpy()\n",
    "\n",
    "            if columns == \"wind_speed\" or columns == \"both\":\n",
    "                d[\"WS10\"] = np.sqrt(np.square(w_u_10) + np.square(w_v_10))\n",
    "\n",
    "            if columns == \"wind_direction\" or columns == \"both\":\n",
    "                d[\"WS10_angle\"] = np.arctan2(w_v_10, w_u_10)\n",
    "\n",
    "        if speed_height == \"100\" or speed_height == \"both\":\n",
    "            w_u_100 = d[\"U100\"].to_numpy()\n",
    "            w_v_100 = d[\"V100\"].to_numpy()\n",
    "\n",
    "            if columns == \"wind_speed\" or columns == \"both\":\n",
    "                d[\"WS100\"] = np.sqrt(np.square(w_u_100) + np.square(w_v_100))\n",
    "\n",
    "            if columns == \"wind_direction\" or columns == \"both\":\n",
    "                d[\"WS100_angle\"] = np.arctan2(w_v_100, w_u_100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- TEST NORMALIZE_BIS -- \n",
    "df =  copy.deepcopy(dataset_original_X)\n",
    "du =  copy.deepcopy(dataset_original_X)\n",
    "normalize(df,\n",
    "            norm_type = \"mean\",\n",
    "            data_type = \"all\")\n",
    "normalize_bis(du,\n",
    "             norm_type = \"standard\",\n",
    "             data_type = \"all\")\n",
    "print(df[0].head())\n",
    "print(du[0].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- DATA LOADER -- \n",
    "# This class has for purpose to handle the data and make our life easier ! \n",
    "#\n",
    "class dataLoader():\n",
    "    \n",
    "    # Initialization of the loader\n",
    "    def __init__(self, datasets_X, datasets_Y):\n",
    "\n",
    "        # Stores the original, transformed and final datasets\n",
    "        self.original_datasets_X    = datasets_X\n",
    "        self.original_datasets_Y    = datasets_Y\n",
    "        self.transformed_datasets_X = datasets_X\n",
    "        self.transformed_datasets_Y = datasets_Y\n",
    "        self.final_dataset_X        = None\n",
    "        self.final_dataset_Y        = None\n",
    "\n",
    "        # Used to know if datasets have been combined or not\n",
    "        self.isCombined = None\n",
    "\n",
    "    # Used to display the head of the transformed dataset (first set)\n",
    "    def showHeadTransformed(self):\n",
    "        section(\"Dataset - X - Transformed\")\n",
    "        print(self.transformed_datasets_X[0].head())\n",
    "        section(\"Dataset - Y - Transformed\")\n",
    "        print(self.transformed_datasets_Y[0].head())\n",
    "\n",
    "    # Used to split the final dataset into a train and test set (In test set, values for y are equal to -1)\n",
    "    def splitTrainTest(self, save = False, save_dir = \"new_data\"):\n",
    "\n",
    "        # Security\n",
    "        assert self.isCombined != None, \"You must first use self.finalize\"\n",
    "\n",
    "        # Case 1 - Datasets have been combined all together\n",
    "        if self.isCombined == True:\n",
    "            X_train = self.final_dataset_X[self.final_dataset_Y['TARGETVAR'] != -1]\n",
    "            Y_train = self.final_dataset_Y[self.final_dataset_Y['TARGETVAR'] != -1]\n",
    "            X_test  = self.final_dataset_X[self.final_dataset_Y['TARGETVAR'] == -1]\n",
    "            Y_test  = self.final_dataset_Y[self.final_dataset_Y['TARGETVAR'] == -1] # Not useful, I know !\n",
    "\n",
    "        # Case 2 - Datasets are still separated\n",
    "        if self.isCombined == False:\n",
    "            \n",
    "            X_train, Y_train, X_test, Y_test = list(), list(), list(), list()\n",
    "\n",
    "            # Looping over all the small datasets\n",
    "            for x, y in zip(self.final_dataset_X, self.final_dataset_Y):\n",
    "                X_train.append(x[y['TARGETVAR'] != -1])\n",
    "                Y_train.append(y[y['TARGETVAR'] != -1])\n",
    "                X_test.append(x[y['TARGETVAR'] == -1])\n",
    "                Y_test.append(y[y['TARGETVAR'] == -1])\n",
    "\n",
    "        # Be careful with the order\n",
    "        return X_train, X_test, Y_train, Y_test\n",
    "        \n",
    "    # Used to perfom final operation on dataset (Combining everything or storing them separately)\n",
    "    def finalization(self, dataset_type = \"combined\"):\n",
    "\n",
    "        # Security\n",
    "        assert dataset_type in [\"combined\", \"separated\"], \"The final dataset can either be of type combined or separated\"\n",
    "\n",
    "        # Case 1 - Combining into one big dataset\n",
    "        if dataset_type == \"combined\":\n",
    "            self.final_dataset_X = pd.concat(self.transformed_datasets_X)\n",
    "            self.final_dataset_Y = pd.concat(self.transformed_datasets_Y)\n",
    "            self.isCombined = True\n",
    "\n",
    "        # Case 2 - Separated datasets\n",
    "        else:\n",
    "            self.final_dataset_X = self.transformed_datasets_X\n",
    "            self.final_dataset_Y = self.transformed_datasets_Y\n",
    "            self.isCombined = False\n",
    "\n",
    "    #--------------------------------------------------------------------------------\n",
    "    #                                    PIPELINES\n",
    "    #--------------------------------------------------------------------------------\n",
    "    #\n",
    "    # List of functions available:\n",
    "    #\n",
    "    # - computeMeanVariance(datasets, variables = ≈, window = 100, variance  = True)\n",
    "    #\n",
    "    # - computeZonalValue(datasets, variables = [\"U100\", \"V100\"], variance  = True)\n",
    "    # \n",
    "    # - addPastTime(datasets, variables = [\"U100\", \"V100\"], window = 3):\n",
    "    #\n",
    "    # - normalize(datasets, norm_type = \"argmax\", data_type = \"column\", variables = [\"U100\", \"V100\"])\n",
    "    #\n",
    "    def pipeline(self, useMeanVariance   = True,  var_MV   = [\"U10\", \"V10\", \"U100\", \"V100\"], variance_MV  = True, window_MV = 24 * 7,\n",
    "                       useZonal          = True,  var_ZON  = [\"U10\", \"V10\", \"U100\", \"V100\"], variance_ZON = True,\n",
    "                       usePastTime       = True,  var_PT   = [\"U10\", \"V10\", \"U100\", \"V100\"], window_ZON   = 3,\n",
    "                       useNormalize      = True,  var_NORM = [\"U10\", \"V10\", \"U100\", \"V100\"], norm_type = \"max_abs\", data_type = \"column\",\n",
    "                       useSpeedNorm      = True,  SpeedNorm_height = \"both\",\n",
    "                       useSpeedDirection = True,  SpeedDir_height = \"both\",\n",
    "                       removing          = False, var_removed = [\"U10\", \"V10\", \"U100\", \"V100\"]):\n",
    "\n",
    "        # Copying original dataset\n",
    "        dX = copy.deepcopy(self.original_datasets_X)\n",
    "        dY = copy.deepcopy(self.original_datasets_Y)\n",
    "\n",
    "        # Applying the different transformations\n",
    "        if useNormalize:\n",
    "            #normalize(dX, variables = var_NORM, norm_type = norm_type, data_type = data_type)\n",
    "            normalize_bis(dX, variables = var_NORM, norm_type = norm_type, data_type = data_type)\n",
    "        if useSpeedNorm:\n",
    "            wind_comp(dX, columns = \"wind_speed\", speed_height = SpeedNorm_height)\n",
    "        if useSpeedDirection:\n",
    "            wind_comp(dX, columns = \"wind_direction\", speed_height = SpeedDir_height)\n",
    "        if useMeanVariance:\n",
    "            computeMeanVariance(dX, variables = var_MV, window = window_MV, variance = variance_MV)\n",
    "        if useZonal:\n",
    "            computeZonalValue(dX, variables = var_ZON, variance = variance_ZON)\n",
    "        if usePastTime:\n",
    "            addPastTime(dX, variables = var_PT, window = window_ZON)\n",
    "        if removing:\n",
    "            remove(dX, variables = var_removed)\n",
    "        \n",
    "        # Updating dataset\n",
    "        self.transformed_datasets_X = dX\n",
    "        self.transformed_datasets_Y = dY\n",
    "\n",
    "        # Making sure one has to finalize again\n",
    "        self.isCombined = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bf4e518",
   "metadata": {},
   "source": [
    "[comment]: <> (Section)\n",
    "<hr style=\"color:#a4342d;\"></hr>\n",
    "<p align=\"center\">\n",
    "    <b style=\"font-size:1.5vw; color:#a4342d;\">\n",
    "    Dataset - Creation of the pre-processed datasets\n",
    "    </b>\n",
    "</p>\n",
    "<hr style=\"color:#a4342d;\"></hr>\n",
    "\n",
    "[comment]: <> (Description)\n",
    "<p align=\"justify\">\n",
    "    In this section, one creates the different datasets on which the models are going to be trained\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- TEMPLATE PIPELINE -- \n",
    "def pipeline(self, useMeanVariance   = True, var_MV   = [\"U10\", \"V10\", \"U100\", \"V100\"], variance_MV  = True, window_MV = 24 * 7,\n",
    "                   useZonal          = True, var_ZON  = [\"U10\", \"V10\", \"U100\", \"V100\"], variance_ZON = True,\n",
    "                   usePastTime       = True, var_PT   = [\"U10\", \"V10\", \"U100\", \"V100\"], window_ZON   = 3,\n",
    "                   useNormalize      = True, var_NORM = [\"U10\", \"V10\", \"U100\", \"V100\"], norm_type = \"max_abs\", data_type = \"column\",\n",
    "                   useSpeedNorm      = True, SpeedNorm_height = \"both\",\n",
    "                   useSpeedDirection = True, SpeedDir_height = \"both\"):\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0762813",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- Dataset 0 : ORIGINAL DATASET --\n",
    "loader = dataLoader(dataset_original_X, dataset_original_Y)\n",
    "loader.pipeline(useMeanVariance   = False,\n",
    "                useZonal          = False,\n",
    "                usePastTime       = False,\n",
    "                useNormalize      = False,\n",
    "                useSpeedNorm      = False,\n",
    "                useSpeedDirection = False)\n",
    "                \n",
    "loader.finalization()\n",
    "\n",
    "data_X0, submit_X0, data_Y0, submit_Y0 = loader.splitTrainTest()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- NORMALIZATION DATASETS --\n",
    "\n",
    "data_X_tot = [data_X0]\n",
    "data_Y_tot = [data_Y0]\n",
    "\n",
    "submit_X_tot = [submit_X0]\n",
    "submit_Y_tot = [submit_Y0]\n",
    "\n",
    "norm_types = [\"standard\", \"max_abs\", \"robust\"]\n",
    "data_types = [\"column\", \"all\"]\n",
    "\n",
    "for d in data_types:\n",
    "    for n in norm_types:\n",
    "     \n",
    "        loader = dataLoader(dataset_original_X, dataset_original_Y)\n",
    "        loader.pipeline(useMeanVariance   = False,\n",
    "                        useZonal          = False,\n",
    "                        usePastTime       = False,\n",
    "                        useNormalize      = True, norm_type = n, data_type = d,\n",
    "                        useSpeedNorm      = False,\n",
    "                        useSpeedDirection = False\n",
    "                        )\n",
    "                        \n",
    "        loader.finalization()\n",
    "\n",
    "        data_X, submit_X, data_Y, submit_Y = loader.splitTrainTest()\n",
    "\n",
    "        data_X_tot.append(data_X)\n",
    "        data_Y_tot.append(data_Y)\n",
    "        submit_X_tot.append(submit_X)\n",
    "        submit_Y_tot.append(submit_Y)  \n",
    "\n",
    "# CONCLUSION: tests made on random forest and knn :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- TYPES OF WIND SPEED DATASETS --\n",
    "\n",
    "norm = True\n",
    "\n",
    "data_X_tot = []\n",
    "data_Y_tot = []\n",
    "\n",
    "submit_X_tot = []\n",
    "submit_Y_tot = []\n",
    "\n",
    "SpeedNorms = [True, False]\n",
    "SpeedDirs = [True, False]\n",
    "\n",
    "var_NORM = [\"U10\", \"V10\", \"U100\", \"V100\"]\n",
    "for sn in SpeedNorms:\n",
    "    if sn:\n",
    "        var_NORM.append(\"WS10\")\n",
    "        var_NORM.append(\"WS100\")\n",
    "    for sp in SpeedDirs:\n",
    "        if sp:\n",
    "            var_NORM.append(\"WS10_angle\")\n",
    "            var_NORM.append(\"WS100_angle\")\n",
    "\n",
    "   \n",
    "        loader = dataLoader(dataset_original_X, dataset_original_Y)\n",
    "        loader.pipeline(useMeanVariance   = False,\n",
    "                        useZonal          = False,\n",
    "                        usePastTime       = False,\n",
    "                        useNormalize      = norm, norm_type=\"standard\", data_type=\"all\",\n",
    "                        useSpeedNorm      = sn,\n",
    "                        useSpeedDirection = sp\n",
    "                        )\n",
    "                        \n",
    "        loader.finalization()\n",
    "\n",
    "        data_X, submit_X, data_Y, submit_Y = loader.splitTrainTest()\n",
    "\n",
    "        data_X_tot.append(data_X)\n",
    "        data_Y_tot.append(data_Y)\n",
    "        submit_X_tot.append(submit_X)\n",
    "        submit_Y_tot.append(submit_Y)  \n",
    "\n",
    "# + test when we remove and train only on wind speed and direction\n",
    "loader = dataLoader(dataset_original_X, dataset_original_Y)\n",
    "loader.pipeline(useMeanVariance   = False,\n",
    "                useZonal          = False,\n",
    "                usePastTime       = False,\n",
    "                useNormalize      = norm, norm_type=\"standard\", data_type=\"column\",\n",
    "                useSpeedNorm      = True,\n",
    "                useSpeedDirection = True,\n",
    "                removing = True\n",
    "                )\n",
    "                        \n",
    "loader.finalization()\n",
    "\n",
    "data_X, submit_X, data_Y, submit_Y = loader.splitTrainTest()\n",
    "\n",
    "data_X_tot.append(data_X)\n",
    "data_Y_tot.append(data_Y)\n",
    "submit_X_tot.append(submit_X)\n",
    "submit_Y_tot.append(submit_Y)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c6b1cb5f",
   "metadata": {},
   "source": [
    "[comment]: <> (Section)\n",
    "<hr style=\"color:#a4342d;\"></hr>\n",
    "<p align=\"center\">\n",
    "    <b style=\"font-size:1.5vw; color:#a4342d;\">\n",
    "    Model - Training & Testing\n",
    "    </b>\n",
    "</p>\n",
    "<hr style=\"color:#a4342d;\"></hr>\n",
    "\n",
    "[comment]: <> (Description)\n",
    "<p align=\"justify\">\n",
    "    In this section, one will be able to explore further the dataset ! First, one needs to create a train and test set. Then, it is interesitng to look for correlations, new variables and possible improvements to the current dataset. All the new datasets will be save in the datafolder and ready to use by our different models ! The functions available are:\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4239fcaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- FUNCTIONS --\n",
    "#\n",
    "# Used to compute a model's accuracy against different datasets\n",
    "def modelTesting(datasets_X, datasets_y, model, test_size = 0.3, random_state = 69):\n",
    "    \n",
    "    # Contains mean accuracy of the model against each dataset\n",
    "    accuracy_train = []\n",
    "    accuracy_test = []\n",
    "\n",
    "    # Looping over whole the different datasets\n",
    "    for X, y in zip(datasets_X, datasets_y):\n",
    "        \n",
    "        # Final conversion (Numpy and retrieving targets)\n",
    "        X = X.to_numpy()\n",
    "        y = y[[\"TARGETVAR\"]].to_numpy().ravel()\n",
    "\n",
    "        # Retrieving datasets\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = test_size, random_state = random_state)\n",
    "\n",
    "        # Fitting the model on current split\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        \n",
    "        # Accuracy\n",
    "        accuracy_train.append(model.score(X_train, y_train))\n",
    "        accuracy_test.append(model.score(X_test, y_test))\n",
    "\n",
    "    return accuracy_train, accuracy_test\n",
    "\n",
    "def modelPlotResults(parameters, acc_train, acc_test, xlabel = \"UNKNOWN\", param_name = \"UNKNOWN\", fontsize = 15, save_path = \"graphs/\"):\n",
    "\n",
    "    # 1 - Evolution of the test accuracy\n",
    "    plt.figure(figsize=(15,8))\n",
    "    sns.set(font_scale = 2)\n",
    "\n",
    "    # Plotting evolution curve for a specific dataset with varying parameter value\n",
    "    for i, a in enumerate(acc_test):\n",
    "        plt.plot(parameters, [a_i * 100 for a_i in a], label = f\"Dataset n°{i}\", linewidth = 5)\n",
    "    \n",
    "    plt.legend(loc=\"upper right\", fontsize = fontsize)\n",
    "    plt.ylabel(\"Accuracy [%]\", fontsize = fontsize)\n",
    "    plt.xlabel(xlabel, fontsize = fontsize)\n",
    "    plt.savefig(f\"{save_path}_1.png\")\n",
    "    plt.show()\n",
    "\n",
    "    # 2 - Bar plot\n",
    "    acc_test_best  = []\n",
    "    acc_train_best = []\n",
    "    k_best         = []\n",
    "\n",
    "    # Looping over accuracies to find best results\n",
    "    for a1, a2 in zip(acc_train, acc_test):\n",
    "\n",
    "        # Finding best test accuracy\n",
    "        max_value = max(a2)\n",
    "        max_index = a2.index(max_value)\n",
    "\n",
    "        # Adding results\n",
    "        acc_test_best.append(max_value)\n",
    "        acc_train_best.append(a1[max_index])\n",
    "        k_best.append(parameters[max_index])\n",
    "\n",
    "    # Contains x-axis labels\n",
    "    x_ax_labels = [f\"Dataset n°{i} - {param_name} = {k_best[i]}\" for i in range(len(acc_train))]\n",
    "\n",
    "    # Used to make x-axis    \n",
    "    index = [i for i in range(len(acc_train))]\n",
    "\n",
    "    # Plotting the results\n",
    "    plt.figure(figsize=(15,8))\n",
    "    plt.bar([i - 0.2 for i in index], [a_i * 100 for a_i in acc_train_best], 0.4, label = \"Train\")\n",
    "    plt.bar([i + 0.2 for i in index], [a_i * 100 for a_i in acc_test_best], 0.4, label = \"Test\")\n",
    "    plt.xticks(index, x_ax_labels)\n",
    "    plt.ylabel(\"Accuracy [%]\", fontsize = fontsize)\n",
    "    plt.legend()\n",
    "    plt.savefig(f\"{save_path}_2.png\")\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"color:#a4342d; width: 180px;\" align=\"left\">\n",
    "<p style=\"color:#a4342d;\">Test normalization datasets</p>\n",
    "<hr style=\"color:#a4342d; width: 180px;\" align=\"left\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- Generating results KNN -- \n",
    "\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "# Definition of the parameters to be tested\n",
    "k_param = np.arange(1, 100, 10, dtype = int)\n",
    "\n",
    "# Stores the accuracy of the training and testing\n",
    "knn_accuracy_train = [[] for i in range(len(data_X_tot))]\n",
    "knn_accuracy_test  = [[] for i in range(len(data_X_tot))]\n",
    "\n",
    "for k in k_param:\n",
    "\n",
    "    # Initialization of the model\n",
    "    model = KNeighborsRegressor(n_neighbors = k)\n",
    "\n",
    "    # Computing accuracies on all the datasets\n",
    "    acc_train, acc_test = modelTesting(data_X_tot, data_Y_tot, model, test_size = 0.3, random_state = 69)\n",
    "\n",
    "    # Adding the results\n",
    "    for i, acc_1, acc_2 in zip(range(len(acc_train)), acc_train, acc_test):\n",
    "        knn_accuracy_train[i].append(acc_1)\n",
    "        knn_accuracy_test[i].append(acc_2)\n",
    "\n",
    "# Plotting the results\n",
    "modelPlotResults(k_param, knn_accuracy_train, knn_accuracy_test, \n",
    "                 xlabel = \"Number of neighbors - $k$ [-]\", fontsize = 30, \n",
    "                 param_name = \"knn\", save_path = \"graphs/knn/knn_normalized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- Generating results Random Forest -- \n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# Definition of the parameters to be tested\n",
    "max_depth = [2, 4, 8]\n",
    "\n",
    "# Stores the accuracy of the training and testing\n",
    "rf_accuracy_train = [[] for i in range(len(data_X_tot))]\n",
    "rf_accuracy_test  = [[] for i in range(len(data_X_tot))]\n",
    "\n",
    "for d in max_depth:\n",
    "\n",
    "    # Initialization of the model\n",
    "    model = RandomForestRegressor(max_depth = d, n_estimators = 10)\n",
    "\n",
    "    # Computing accuracies on all the datasets\n",
    "    acc_train, acc_test = modelTesting(data_X_tot, data_Y_tot, model, test_size = 0.3, random_state = 69)\n",
    "\n",
    "    # Adding the results\n",
    "    for i, acc_1, acc_2 in zip(range(len(acc_train)), acc_train, acc_test):\n",
    "        rf_accuracy_train[i].append(acc_1)\n",
    "        rf_accuracy_test[i].append(acc_2)\n",
    "\n",
    "# Plotting the results\n",
    "modelPlotResults(max_depth, rf_accuracy_train, rf_accuracy_test, \n",
    "                 xlabel = \"Depth of the tree - $d$ [-]\", fontsize = 30, \n",
    "                 param_name = \"d\", save_path = \"graphs/rf/rf_normalized\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- Generating results Linear Regression -- \n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "\n",
    "acc_train, acc_test = modelTesting(data_X_tot, data_Y_tot, model, test_size = 0.3, random_state = 69)\n",
    "print(acc_train)\n",
    "for i in range(len(data_X_tot)):\n",
    "    print(\"\\n----------- Dataset number %s -----------\\n\" %i)\n",
    "    print(\"Accuracy training set: %s\" %acc_train[i])\n",
    "    print(\"Accuracy test set: %s\" %acc_test[i])\n",
    "\n",
    "\n",
    "# Linear regression seems to prefer the dataset left unchanged (no need to add the wind direction)\n",
    "\n",
    "SpeedNorms = [True, False]\n",
    "SpeedDirs = [True, False]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('Uliege')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6e6efc9069f2759351b6932eeea0e6596aa0b1c86e37a31760593717586e41da"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
